{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d339923-ddf1-461d-bcd9-fca0f4d140cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sentence-transformers==2.2.2 torch huggingface_hub==0.24.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "256124a2-1cd3-441c-a67a-4e0fd393a475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from delta.tables import DeltaTable\n",
    "from typing import Iterator\n",
    "import traceback\n",
    "\n",
    "SILVER_TABLE = \"cvee.jobs_silver\"\n",
    "GOLD_TABLE = \"cvee.jobs_gold\"\n",
    "TEMP_TABLE_NAME = \"pyspark_tmp_embeddings\"\n",
    "OUTPUT_SCHEMA = \"job_id string, ingestion_date string, embedding array<float>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e1acaf-d7c8-4420-93a7-1bef54bcf818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(\"array<float>\")\n",
    "def get_embeddings_iterator(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"\n",
    "    Pandas UDF that generates text embeddings for batches of input strings using a SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ['USER'] = 'ubuntu'\n",
    "    os.environ['HOME'] = '/tmp'\n",
    "    os.environ['TRANSFORMERS_CACHE'] = '/tmp/huggingface'\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # CPU Optimization\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    # Model loading\n",
    "    model = SentenceTransformer(\"BAAI/bge-small-en\", device=\"cpu\")\n",
    "    for batch in batch_iter:\n",
    "        texts = batch.fillna(\"\").str.slice(0, 5000).tolist()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(\n",
    "                texts, \n",
    "                batch_size=16, \n",
    "                show_progress_bar=False, \n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            \n",
    "        yield pd.Series(embeddings.tolist())\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del texts\n",
    "        del embeddings\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f4b16b-3d5c-49f3-8f86-79e96374d2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "df_silver = spark.table(SILVER_TABLE).select(\"job_id\", \"vector_text_input\", \"ingestion_date\")\n",
    "df_gold = spark.table(GOLD_TABLE).select(\"job_id\")\n",
    "df_to_process = df_silver.join(df_gold, on=\"job_id\", how=\"left_anti\")\n",
    "\n",
    "df_prepared = df_to_process.select(\n",
    "    \"job_id\", \n",
    "    \"ingestion_date\", \n",
    "    F.coalesce(F.col(\"vector_text_input\"), F.lit(\"\")).alias(\"text_input\")\n",
    ").repartition(100)\n",
    "\n",
    "display(f\"{df_to_process.count()} new offers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c7425a-fea7-481d-b355-679f72f56d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execution\n",
    "\n",
    "#Applying UDF\n",
    "df_final = df_prepared.withColumn(\n",
    "    \"embedding\", \n",
    "    get_embeddings_iterator(F.col(\"text_input\"))\n",
    ").select(\"job_id\", \"ingestion_date\", \"embedding\")\n",
    "\n",
    "print(\"Generating vectors...\")\n",
    "\n",
    "\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(TEMP_TABLE_NAME)\n",
    "\n",
    "print(f\"Embeddings successfully saved to {TEMP_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f11fea3-8ecb-4d18-bab8-f9e58f77bd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Merging data from {TEMP_TABLE_NAME} to {GOLD_TABLE}...\")\n",
    "\n",
    "try:\n",
    "    gold_table = DeltaTable.forName(spark, GOLD_TABLE)\n",
    "    df_temp = spark.table(TEMP_TABLE_NAME)\n",
    "    gold_table.alias(\"target\") \\\n",
    "        .merge(\n",
    "            df_temp.alias(\"source\"),\n",
    "            \"target.job_id = source.job_id\"\n",
    "        ) \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "\n",
    "    print(\"Merge completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n Merge failed\")\n",
    "    print(f\"Error Type : {type(e).__name__}\")\n",
    "    print(f\"Error Message : {e}\")\n",
    "    print(\"\\nStacktrace\")\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7375049592732470,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "jobs_embeddings_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
