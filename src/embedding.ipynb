{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5988b02d-c3bf-4cfd-983a-92b964ef6c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install sentence-transformers==2.2.2 torch --quiet\n",
    "%pip install \"huggingface_hub<=0.24.0\" \"sentence-transformers>=2.6.1\"\n",
    "\n",
    "\n",
    "%restart_python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b166ecb4-612c-4c16-afef-b7bbdd1192a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, col, substring\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae17a3d-bd63-4099-9d9a-509bb532779f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"BAAI/bge-small-en\"\n",
    "print(\"Model loading:\", MODEL_NAME)\n",
    "model = SentenceTransformer(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "EMBED_DIM = len(model.encode(\"test\").tolist())\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def embedding_pandas_udf(texts: pd.Series) -> pd.Series:\n",
    "    texts_filled = texts.fillna(\"\")\n",
    "    # Encoding \n",
    "    embeddings = model.encode(texts_filled.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "    return pd.Series([emb.astype(float).tolist() for emb in embeddings])\n",
    "\n",
    "\n",
    "silver_table = \"cvee.job_metadata_silver\"\n",
    "silver_df = spark.table(silver_table).orderBy(\"job_id\")\n",
    "df_reduced = silver_df.select([\"job_id\", \"vector_text_input\"])\n",
    "df_prepared = df_reduced.withColumn(\"vector_text_input\", F.when(col(\"vector_text_input\").isNull(), F.lit(\"\")).otherwise(col(\"vector_text_input\")))\n",
    "\n",
    "NUM_PARTITIONS = 100\n",
    "df_repart = df_prepared.repartition(NUM_PARTITIONS)\n",
    "print(\"Partitions:\", NUM_PARTITIONS)\n",
    "\n",
    "gold_df = df_repart.withColumn(\"embedding\", embedding_pandas_udf(F.col(\"vector_text_input\")))\n",
    "golf_df=gold_df.select(\"job_id\", \"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b707b4-09d3-422c-b09a-d7c1f050286c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GOLD_TABLE_PATH = \"cvee.job_embedding_gold\"\n",
    "\n",
    "try:\n",
    "    delta_table = DeltaTable.forName(spark, GOLD_TABLE_PATH)\n",
    "    old_count = delta_table.toDF().count()\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        golf_df.alias(\"source\"),\n",
    "        \"target.job_id = source.job_id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    new_count = delta_table.toDF().count()\n",
    "    print(f\"Number of rows added: {new_count - old_count} / {golf_df.count()} read\")\n",
    "except:\n",
    "    golf_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_TABLE_PATH)\n",
    "    print(f\"Table created with {golf_df.count()} rows\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5681216792168780,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "embedding",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
