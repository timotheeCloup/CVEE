{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89021973-bdb7-4de5-96e9-1a2efb1a5380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pyspark.sql.functions import udf, col, concat_ws, lit, when, current_date\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658f1b78-162d-49f6-a264-43c927436c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Read data\")\n",
    "\n",
    "S3_RAW_PATH = \"s3://cvee-bucket-eu-north-1/jobs_raw/\"\n",
    "DELTA_TABLE_PATH = \"cvee.jobs_silver\"\n",
    "\n",
    "files = dbutils.fs.ls(S3_RAW_PATH)\n",
    "parquet_files = [f.path for f in files if f.path.endswith(\".parquet\")]\n",
    "\n",
    "\n",
    "def extract_date_from_name(path):\n",
    "    match = re.search(r\"jobs_raw_(\\d{8}_\\d{6})\\.parquet\", path)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(1), \"%Y%m%d_%H%M%S\")\n",
    "    return datetime.min\n",
    "\n",
    "latest_file_path = max(parquet_files, key=extract_date_from_name)\n",
    "\n",
    "print(f\"Reading data from S3 {latest_file_path} \")\n",
    "df_raw = spark.read.parquet(latest_file_path)\n",
    "print(f\"Number of records read: {df_raw.count()}\")\n",
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2556ce74-d7ab-4b9e-a1b7-2e8911979767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Transform data\")\n",
    "\n",
    "# User defined functions\n",
    "\n",
    "def clean_html(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    clean = re.compile('<.*?>|&([a-zA-Z0-9]+|#[0-9]{1,6}|#x[0-9a-fA-F]{1,6});')\n",
    "    text = re.sub(clean, '', text)\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "clean_html_udf = udf(clean_html, StringType())\n",
    "\n",
    "df_cleaned = df_raw \\\n",
    "    .withColumn(\"description_clean\", clean_html_udf(col(\"description\"))) \\\n",
    "    .withColumn(\"competences_aggregated\", \n",
    "                F.expr(\"concat_ws(' ', transform(competences, x -> x.libelle))\")) \\\n",
    "    .withColumn(\"formations_aggregated\", \n",
    "                F.expr(\"concat_ws(' ', transform(formations, x -> x.domaineLibelle))\")) \\\n",
    "    .withColumn(\"qualites_aggregated\", \n",
    "                F.expr(\"concat_ws(' ', transform(qualitesProfessionnelles, x -> x.libelle))\")) \\\n",
    "\n",
    "\n",
    "df_final = df_cleaned.withColumn(\n",
    "    \"vector_text_input\",\n",
    "    concat_ws(\n",
    "        \" \",\n",
    "        col(\"intitule\"),\n",
    "        col(\"description_clean\"),\n",
    "        col(\"competences_aggregated\"),\n",
    "        col(\"formations_aggregated\"),\n",
    "        col(\"qualites_aggregated\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_final = df_final.dropDuplicates([\"id\"])\n",
    "\n",
    "df_final = df_final.drop(\"description\", \"competences_aggregated\", \"formations_aggregated\", \"qualites_aggregated\")\n",
    "\n",
    "df_final = df_final.withColumnRenamed(\"id\", \"job_id\") \\\n",
    "                   .withColumnRenamed(\"description_clean\", \"description\") \\\n",
    "                   .withColumn(\"ingestion_date\", current_date()) \\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36e5f80-9c50-4d2e-b046-6face876adb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Translation with ai_translate\")\n",
    "\n",
    "df_translated = df_final.withColumn(\n",
    "    \"vector_text_input_en\",\n",
    "    F.expr(\"ai_translate(vector_text_input, 'en')\")\n",
    ").drop(\"vector_text_input\") \\\n",
    "    .withColumnRenamed(\"vector_text_input_en\", \"vector_text_input\")\n",
    "\n",
    "print(df_translated.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dcc4630-ba35-468d-91f7-018ab29ce022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Merging into Delta table {DELTA_TABLE_PATH}\")\n",
    "try:\n",
    "    # Handle column mismatch\n",
    "    target_schema = spark.table(DELTA_TABLE_PATH).schema\n",
    "    df_source_aligned = df_translated.select(\n",
    "        F.from_json(F.to_json(F.struct(\"*\")), target_schema).alias(\"data\")\n",
    "    ).select(\"data.*\")\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, DELTA_TABLE_PATH)\n",
    "    old_count = delta_table.toDF().count()\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        df_source_aligned.alias(\"source\"),\n",
    "        \"target.job_id = source.job_id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    new_count = delta_table.toDF().count()\n",
    "    print(f\"Number of rows added: {new_count - old_count} / {df_source_aligned.count()} read\")\n",
    "except Exception as e:\n",
    "    #df_translated.write.format(\"delta\").mode(\"overwrite\").saveAsTable(DELTA_TABLE_PATH)\n",
    "    #print(f\"Table created with {df_translated.count()} rows\")\n",
    "    print(\"\\n Merge failed\")\n",
    "    print(f\"Error Type : {type(e).__name__}\")\n",
    "    print(f\"Error Message : {e}\")\n",
    "    print(\"\\nStacktrace\")\n",
    "    traceback.print_exc()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "jobs_ingestion_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
