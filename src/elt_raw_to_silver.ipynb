{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2126e2d-e118-479e-a73a-753465856fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pyspark.sql.functions import udf, col, concat_ws, lit, when\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "S3_RAW_PATH = \"s3://cvee-bucket-eu-north-1/jobs_metadata_raw/jobs_metadata_raw_20251204_160713.csv\"\n",
    "CSV_OUTPUT_PATH = \"/Volumes/workspace/cvee/raw_to_silver_etl/jobs_metadata_silver\"\n",
    "TARGET_TABLE = \"cvee.job_metadata_silver\"\n",
    "\n",
    "# User defined functions\n",
    "\n",
    "def clean_html(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    clean = re.compile('<.*?>|&([a-zA-Z0-9]+|#[0-9]{1,6}|#x[0-9a-fA-F]{1,6});')\n",
    "    text = re.sub(clean, '', text)\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_and_aggregate_struct_data(struct_list, key_to_extract):\n",
    "    if struct_list is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        data = json.loads(struct_list) if isinstance(struct_list, str) else struct_list\n",
    "        if isinstance(data, dict):\n",
    "            return str(data.get(key_to_extract, \"\"))\n",
    "        if isinstance(data, list):\n",
    "            labels = [item.get(key_to_extract, \"\") for item in data if isinstance(item, dict)]\n",
    "            return \" \".join(labels).strip()\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "clean_html_udf = udf(clean_html, StringType())\n",
    "aggregate_data_udf = udf(extract_and_aggregate_struct_data, StringType())\n",
    "\n",
    "\n",
    "def run_spark_etl():\n",
    "    print(f\"Reading data from S3 {S3_RAW_PATH} ---\")\n",
    "    df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(S3_RAW_PATH)\n",
    "    print(f\"Number of records read: {df_raw.count()}\")\n",
    "    df_raw.printSchema()\n",
    "\n",
    "    print(\"Stage 2: Transform data\")\n",
    "    df_cleaned = df_raw \\\n",
    "        .withColumn(\"description_clean\", clean_html_udf(col(\"description\"))) \\\n",
    "        .withColumn(\"competences_aggregated\", aggregate_data_udf(col(\"competences\"), lit(\"libelle\"))) \\\n",
    "        .withColumn(\"formations_aggregated\", aggregate_data_udf(col(\"formations\"), lit(\"domaineLibelle\"))) \\\n",
    "        .withColumn(\"qualites_aggregated\", aggregate_data_udf(col(\"qualitesprofessionnelles\"), lit(\"libelle\"))) \\\n",
    "        .withColumn(\"city_label\", aggregate_data_udf(col(\"lieutravail\"), lit(\"libelle\")))\n",
    "\n",
    "    df_final = df_cleaned.withColumn(\n",
    "        \"vector_text_input\",\n",
    "        concat_ws(\n",
    "            \" \",\n",
    "            col(\"intitule\"),\n",
    "            col(\"description_clean\"),\n",
    "            col(\"competences_aggregated\"),\n",
    "            col(\"formations_aggregated\"),\n",
    "            col(\"qualites_aggregated\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_final = df_final.select(\n",
    "        col(\"id\").alias(\"job_id\"),\n",
    "        col(\"intitule\").alias(\"job_title\"),\n",
    "        col(\"city_label\").alias(\"job_location\"),\n",
    "        \"vector_text_input\",\n",
    "        col(\"description_clean\"),\n",
    "        col(\"typecontratlibelle\").alias(\"contract_type_label\"),\n",
    "        col(\"experiencelibelle\").alias(\"experience_level_label\"),\n",
    "        col(\"romecode\").alias(\"rome_code\"),\n",
    "        col(\"secteuractivitelibelle\").alias(\"sector_label\"),\n",
    "        col(\"datecreation\").alias(\"created_at\"),\n",
    "        when(col(\"alternance\") == \"False\", lit(0)).otherwise(lit(1)).alias(\"is_alternance\"),\n",
    "        when(col(\"accessibleth\") == \"True\", lit(1)).otherwise(lit(0)).alias(\"is_accessible_th\")\n",
    "    )\n",
    "\n",
    "    print(f\"Load {TARGET_TABLE} ---\")\n",
    "    df_final.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(TARGET_TABLE)\n",
    "\n",
    "    spark.sql( f\"\"\"\n",
    "        SELECT job_id, job_title, SUBSTRING(vector_text_input, 1, 120) AS preview\n",
    "        FROM {TARGET_TABLE}\n",
    "        LIMIT 5\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    print(f\"--- Writing CSV output to {CSV_OUTPUT_PATH} ---\")\n",
    "    df_final.write.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(CSV_OUTPUT_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_spark_etl()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "elt_raw_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
